{"paragraphs":[{"text":"%pyspark\nimport pyspark\nfrom pyspark.sql import SparkSession\n\nspark = SparkSession.builder.appName(\"sample\").master(\"yarn[*]\").getOrCreate()\nspark.conf.set( \"spark.sql.crossJoin.enabled\" , \"true\")\nspark.conf.set(\"spark.memory.offHeap.enabled\",\"true\")\nspark.conf.set(\"spark.memory.offHeap.size\",\"10g\")\n\n\nmain = spark.read.csv(\"hdfs:///user/maria_dev/pre/gmain.csv\", header=True, sep=\",\", inferSchema=True)\noil = spark.read.csv(\"hdfs:///user/maria_dev/pre/goil.csv\", header=True, sep=\",\", inferSchema=True)\nelectricity = spark.read.csv(\"hdfs:///user/maria_dev/pre/gelectricity.csv\", header=True, sep=\",\", inferSchema=True)\nweather = spark.read.csv(\"hdfs:///user/maria_dev/pre/gweathertotal.csv\", header=True, sep=\",\", inferSchema=True)\nimportindex = spark.read.csv(\"hdfs:///user/maria_dev/pre/gimportindex.csv\", header=True, sep=\",\", inferSchema=True)\nintroduction = spark.read.csv(\"hdfs:///user/maria_dev/pre/gintroduction.csv\", header=True, sep=\",\", inferSchema=True)\nlng = spark.read.csv(\"hdfs:///user/maria_dev/pre/glng.csv\", header=True, sep=\",\", inferSchema=True)\n","user":"anonymous","dateUpdated":"2022-12-11T17:54:02+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=173","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=174","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=175","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=176","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=177","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=178","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=179","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=180","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=181","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=182","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=183","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=184","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=185","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=186"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1670776697219_-1639870650","id":"20221211-163817_1620349792","dateCreated":"2022-12-11T16:38:17+0000","dateStarted":"2022-12-11T17:54:02+0000","dateFinished":"2022-12-11T17:54:13+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:9068"},{"text":"%pyspark\nmain.show(2)","user":"anonymous","dateUpdated":"2022-12-11T17:54:27+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------------------+---------+-------------+--------+---------+----------+--------+\n|_c0|          main_date|main_time|main_division|  supply|main_year|main_month|main_day|\n+---+-------------------+---------+-------------+--------+---------+----------+--------+\n|  0|2013-01-01 00:00:00|        1|            0|2497.129|     2013|         1|       1|\n|  1|2013-01-01 00:00:00|        2|            0|2363.265|     2013|         1|       1|\n+---+-------------------+---------+-------------+--------+---------+----------+--------+\nonly showing top 2 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=187"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1670768309681_1084987658","id":"20221211-141829_1411315166","dateCreated":"2022-12-11T14:18:29+0000","dateStarted":"2022-12-11T17:54:27+0000","dateFinished":"2022-12-11T17:54:27+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9069"},{"text":"%pyspark\nfrom pyspark.sql.functions import expr\n\njoinExpression = ((main[\"main_year\"] == importindex[\"import_year\"]) & (main[\"main_month\"] == importindex[\"import_month\"]))\njoinType = \"inner\"\nmain = main.join(importindex, joinExpression, joinType)","user":"anonymous","dateUpdated":"2022-12-11T17:54:29+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1670768657521_1048685528","id":"20221211-142417_795594822","dateCreated":"2022-12-11T14:24:17+0000","dateStarted":"2022-12-11T17:54:29+0000","dateFinished":"2022-12-11T17:54:29+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9070"},{"text":"%pyspark\nmain.show(2)","user":"anonymous","dateUpdated":"2022-12-11T17:54:31+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------------------+---------+-------------+--------+---------+----------+--------+---+----------+----------+------------+------------+-----------+------------+\n|_c0|          main_date|main_time|main_division|  supply|main_year|main_month|main_day|_c0|import_oil|import_lng|import_crude|import_liqud|import_year|import_month|\n+---+-------------------+---------+-------------+--------+---------+----------+--------+---+----------+----------+------------+------------+-----------+------------+\n|  0|2013-01-01 00:00:00|        1|            0|2497.129|     2013|         1|       1|  0|    190.46|    151.82|      196.33|      223.27|       2013|           1|\n|  1|2013-01-01 00:00:00|        2|            0|2363.265|     2013|         1|       1|  0|    190.46|    151.82|      196.33|      223.27|       2013|           1|\n+---+-------------------+---------+-------------+--------+---------+----------+--------+---+----------+----------+------------+------------+-----------+------------+\nonly showing top 2 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=188","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=189"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1670774143249_-405052549","id":"20221211-155543_1307538148","dateCreated":"2022-12-11T15:55:43+0000","dateStarted":"2022-12-11T17:54:31+0000","dateFinished":"2022-12-11T17:54:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9071"},{"text":"%pyspark\njoinExpression = main[\"main_year\"] == introduction[\"intro_year\"]\njoinType = \"inner\"\nmain = main.join(introduction, joinExpression, joinType)","user":"anonymous","dateUpdated":"2022-12-11T17:54:34+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1670769854281_194016007","id":"20221211-144414_1818941797","dateCreated":"2022-12-11T14:44:14+0000","dateStarted":"2022-12-11T17:54:34+0000","dateFinished":"2022-12-11T17:54:34+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9072"},{"text":"%pyspark\nmain.show(2)","user":"anonymous","dateUpdated":"2022-12-11T17:54:35+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------------------+---------+-------------+--------+---------+----------+--------+---+----------+----------+------------+------------+-----------+------------+---+----------+-----+------+-----------+----------+\n|_c0|          main_date|main_time|main_division|  supply|main_year|main_month|main_day|_c0|import_oil|import_lng|import_crude|import_liqud|import_year|import_month|_c0|intro_year|intro|demand|demand_city|demand_dev|\n+---+-------------------+---------+-------------+--------+---------+----------+--------+---+----------+----------+------------+------------+-----------+------------+---+----------+-----+------+-----------+----------+\n|  0|2013-01-01 00:00:00|        1|            0|2497.129|     2013|         1|       1|  0|    190.46|    151.82|      196.33|      223.27|       2013|           1|  1|      2013|39876| 38675|      19596|     19079|\n|  1|2013-01-01 00:00:00|        2|            0|2363.265|     2013|         1|       1|  0|    190.46|    151.82|      196.33|      223.27|       2013|           1|  1|      2013|39876| 38675|      19596|     19079|\n+---+-------------------+---------+-------------+--------+---------+----------+--------+---+----------+----------+------------+------------+-----------+------------+---+----------+-----+------+-----------+----------+\nonly showing top 2 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=190","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=191","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=192"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1670774557409_1428948574","id":"20221211-160237_1775797367","dateCreated":"2022-12-11T16:02:37+0000","dateStarted":"2022-12-11T17:54:35+0000","dateFinished":"2022-12-11T17:54:36+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9073"},{"text":"%pyspark\njoinExpression = ((main[\"main_year\"] == weather[\"temp_year\"]) & (main[\"main_month\"] == weather[\"temp_month\"]) & (main[\"main_day\"] == weather[\"temp_day\"]) )\njoinType = \"inner\"\nmain = main.join(weather, joinExpression, joinType)\n","user":"anonymous","dateUpdated":"2022-12-11T17:54:59+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1670769916121_-1415222621","id":"20221211-144516_33035448","dateCreated":"2022-12-11T14:45:16+0000","dateStarted":"2022-12-11T17:54:59+0000","dateFinished":"2022-12-11T17:54:59+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9074"},{"text":"%pyspark\njoinExpression = ((main[\"import_year\"] == electricity[\"ele_year\"]) & (main[\"import_month\"] == electricity[\"ele_month\"]))\njoinType = \"inner\"\nmain = main.join(electricity, joinExpression, joinType)\n","user":"anonymous","dateUpdated":"2022-12-11T17:55:01+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1670769923409_-1611994745","id":"20221211-144523_1362595457","dateCreated":"2022-12-11T14:45:23+0000","dateStarted":"2022-12-11T17:55:01+0000","dateFinished":"2022-12-11T17:55:01+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9075"},{"text":"%pyspark\njoinExpression = ((main[\"ele_year\"] == oil[\"oil_year\"]) & (main[\"ele_month\"] == oil[\"oil_month\"]))\njoinType = \"inner\"\nmain = main.join(oil, joinExpression, joinType)\noil.show()","user":"anonymous","dateUpdated":"2022-12-11T17:55:03+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-------------------+--------+-------+----------+--------+---------+-------+-------------+---------+------------------+\n|_c0|           oil_date|oil_time|oil_div|oil_supply|oil_year|oil_month|oil_day|oil_gas_price|oil_price|oil_relative_price|\n+---+-------------------+--------+-------+----------+--------+---------+-------+-------------+---------+------------------+\n|  0|2013-01-01 00:00:00|       1|      0|  2497.129|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  1|2013-01-01 00:00:00|       2|      0|  2363.265|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  2|2013-01-01 00:00:00|       3|      0|  2258.505|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  3|2013-01-01 00:00:00|       4|      0|  2243.969|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  4|2013-01-01 00:00:00|       5|      0|  2344.105|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  5|2013-01-01 00:00:00|       6|      0|  2390.961|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  6|2013-01-01 00:00:00|       7|      0|  2378.457|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  7|2013-01-01 00:00:00|       8|      0|  2518.921|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  8|2013-01-01 00:00:00|       9|      0|  2706.481|    2013|        1|      1|       114.52|    174.1|              0.66|\n|  9|2013-01-01 00:00:00|      10|      0|  2832.057|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 10|2013-01-01 00:00:00|      11|      0|  2895.185|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 11|2013-01-01 00:00:00|      12|      0|  2689.361|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 12|2013-01-01 00:00:00|      13|      0|  2425.537|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 13|2013-01-01 00:00:00|      14|      0|  2254.289|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 14|2013-01-01 00:00:00|      15|      0|  2153.361|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 15|2013-01-01 00:00:00|      16|      0|  2126.969|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 16|2013-01-01 00:00:00|      17|      0|  2210.481|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 17|2013-01-01 00:00:00|      18|      0|  2546.873|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 18|2013-01-01 00:00:00|      19|      0|  2886.097|    2013|        1|      1|       114.52|    174.1|              0.66|\n| 19|2013-01-01 00:00:00|      20|      0|  2863.009|    2013|        1|      1|       114.52|    174.1|              0.66|\n+---+-------------------+--------+-------+----------+--------+---------+-------+-------------+---------+------------------+\nonly showing top 20 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=193"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1670770008641_-1316594653","id":"20221211-144648_1622790497","dateCreated":"2022-12-11T14:46:48+0000","dateStarted":"2022-12-11T17:55:03+0000","dateFinished":"2022-12-11T17:55:03+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9076"},{"text":"%pyspark\njoinExpression = (main[\"ele_year\"] == lng[\"lng_year\"]) \njoinType = \"inner\"\nmain = main.join(lng, joinExpression, joinType)\nlng.show(2)","user":"anonymous","dateUpdated":"2022-12-11T17:55:05+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+---+-----+---------+-----+--------+-----------+------+----+------+------+-----------+-------+---------+--------+\n|_c0|World|Indonesia|Qatar|Malaysia|SaudiArabia|Canada| USA|Norway|Russia|Netherlands|Algeria|Australia|lng_year|\n+---+-----+---------+-----+--------+-----------+------+----+------+------+-----------+-------+---------+--------+\n|  0|121.2|      2.8|  6.0|     2.6|        3.4|   5.5|23.6|   3.9|  22.1|        2.6|    2.9|      2.2|    2013|\n|  1|123.6|      2.8|  6.1|     2.6|        3.5|   5.7|25.4|   3.9|  21.3|        2.2|    2.9|      2.3|    2014|\n+---+-----+---------+-----+--------+-----------+------+----+------+------+-----------+-------+---------+--------+\nonly showing top 2 rows\n\n"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=194"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1670771630409_1433050919","id":"20221211-151350_594338571","dateCreated":"2022-12-11T15:13:50+0000","dateStarted":"2022-12-11T17:55:05+0000","dateFinished":"2022-12-11T17:55:06+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9077"},{"text":"%pyspark\nmain.printSchema()","user":"anonymous","dateUpdated":"2022-12-11T17:55:08+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- _c0: integer (nullable = true)\n |-- main_date: timestamp (nullable = true)\n |-- main_time: integer (nullable = true)\n |-- main_division: integer (nullable = true)\n |-- supply: double (nullable = true)\n |-- main_year: integer (nullable = true)\n |-- main_month: integer (nullable = true)\n |-- main_day: integer (nullable = true)\n |-- _c0: integer (nullable = true)\n |-- import_oil: double (nullable = true)\n |-- import_lng: double (nullable = true)\n |-- import_crude: double (nullable = true)\n |-- import_liqud: double (nullable = true)\n |-- import_year: integer (nullable = true)\n |-- import_month: integer (nullable = true)\n |-- _c0: integer (nullable = true)\n |-- intro_year: integer (nullable = true)\n |-- intro: integer (nullable = true)\n |-- demand: integer (nullable = true)\n |-- demand_city: integer (nullable = true)\n |-- demand_dev: integer (nullable = true)\n |-- _c0: integer (nullable = true)\n |-- temp_date: timestamp (nullable = true)\n |-- temp: double (nullable = true)\n |-- temp_year: integer (nullable = true)\n |-- temp_month: integer (nullable = true)\n |-- temp_day: integer (nullable = true)\n |-- _c0: integer (nullable = true)\n |-- ele_date: timestamp (nullable = true)\n |-- ele_time: integer (nullable = true)\n |-- ele_div: integer (nullable = true)\n |-- ele_supply: double (nullable = true)\n |-- ele_year: integer (nullable = true)\n |-- ele_month: integer (nullable = true)\n |-- ele_day: integer (nullable = true)\n |-- ele_gas_price: double (nullable = true)\n |-- ele_price: double (nullable = true)\n |-- ele_relative_price: double (nullable = true)\n |-- _c0: integer (nullable = true)\n |-- oil_date: timestamp (nullable = true)\n |-- oil_time: integer (nullable = true)\n |-- oil_div: integer (nullable = true)\n |-- oil_supply: double (nullable = true)\n |-- oil_year: integer (nullable = true)\n |-- oil_month: integer (nullable = true)\n |-- oil_day: integer (nullable = true)\n |-- oil_gas_price: double (nullable = true)\n |-- oil_price: double (nullable = true)\n |-- oil_relative_price: double (nullable = true)\n |-- _c0: integer (nullable = true)\n |-- World: double (nullable = true)\n |-- Indonesia: double (nullable = true)\n |-- Qatar: double (nullable = true)\n |-- Malaysia: double (nullable = true)\n |-- SaudiArabia: double (nullable = true)\n |-- Canada: double (nullable = true)\n |-- USA: double (nullable = true)\n |-- Norway: double (nullable = true)\n |-- Russia: double (nullable = true)\n |-- Netherlands: double (nullable = true)\n |-- Algeria: double (nullable = true)\n |-- Australia: double (nullable = true)\n |-- lng_year: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1670779054645_1617107374","id":"20221211-171734_873770299","dateCreated":"2022-12-11T17:17:34+0000","dateStarted":"2022-12-11T17:55:08+0000","dateFinished":"2022-12-11T17:55:08+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9078"},{"text":"%pyspark\nmain = main.select(\"main_date\", \"main_time\", \"main_division\", \"supply\", \"main_year\", \"main_month\",\"import_oil\", \"import_crude\",\"import_liqud\", \"import_year\", \"import_month\", \"intro_year\", \"intro\", \"demand\", \"demand_city\", \"demand_dev\", \"temp_date\", \"temp\", \"temp_year\", \"temp_month\", \"temp_day\", \"ele_date\",\"ele_time\",\"ele_div\",\"ele_supply\",\"ele_year\",\"ele_month\",\"ele_day\",\"ele_gas_price\",\"ele_price\",\"ele_relative_price\", \"oil_date\",\"oil_time\",\"oil_div\",\"oil_supply\",\"oil_year\",\"oil_month\",\"oil_day\",\"oil_gas_price\",\"oil_price\",\"oil_relative_price\",\"World\",\"Indonesia\",\"Qatar\",\"Malaysia\",\"SaudiArabia\",\"Canada\" ,\"USA\",\"Norway\",\"Russia\",\"Netherlands\",\"Algeria\",\"Australia\",\"lng_year\")","user":"anonymous","dateUpdated":"2022-12-11T17:55:10+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1670775350249_-688604673","id":"20221211-161550_262092485","dateCreated":"2022-12-11T16:15:50+0000","dateStarted":"2022-12-11T17:55:10+0000","dateFinished":"2022-12-11T17:55:11+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9079"},{"text":"%pyspark\nmain.printSchema()","user":"anonymous","dateUpdated":"2022-12-11T17:55:12+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"root\n |-- main_date: timestamp (nullable = true)\n |-- main_time: integer (nullable = true)\n |-- main_division: integer (nullable = true)\n |-- supply: double (nullable = true)\n |-- main_year: integer (nullable = true)\n |-- main_month: integer (nullable = true)\n |-- import_oil: double (nullable = true)\n |-- import_crude: double (nullable = true)\n |-- import_liqud: double (nullable = true)\n |-- import_year: integer (nullable = true)\n |-- import_month: integer (nullable = true)\n |-- intro_year: integer (nullable = true)\n |-- intro: integer (nullable = true)\n |-- demand: integer (nullable = true)\n |-- demand_city: integer (nullable = true)\n |-- demand_dev: integer (nullable = true)\n |-- temp_date: timestamp (nullable = true)\n |-- temp: double (nullable = true)\n |-- temp_year: integer (nullable = true)\n |-- temp_month: integer (nullable = true)\n |-- temp_day: integer (nullable = true)\n |-- ele_date: timestamp (nullable = true)\n |-- ele_time: integer (nullable = true)\n |-- ele_div: integer (nullable = true)\n |-- ele_supply: double (nullable = true)\n |-- ele_year: integer (nullable = true)\n |-- ele_month: integer (nullable = true)\n |-- ele_day: integer (nullable = true)\n |-- ele_gas_price: double (nullable = true)\n |-- ele_price: double (nullable = true)\n |-- ele_relative_price: double (nullable = true)\n |-- oil_date: timestamp (nullable = true)\n |-- oil_time: integer (nullable = true)\n |-- oil_div: integer (nullable = true)\n |-- oil_supply: double (nullable = true)\n |-- oil_year: integer (nullable = true)\n |-- oil_month: integer (nullable = true)\n |-- oil_day: integer (nullable = true)\n |-- oil_gas_price: double (nullable = true)\n |-- oil_price: double (nullable = true)\n |-- oil_relative_price: double (nullable = true)\n |-- World: double (nullable = true)\n |-- Indonesia: double (nullable = true)\n |-- Qatar: double (nullable = true)\n |-- Malaysia: double (nullable = true)\n |-- SaudiArabia: double (nullable = true)\n |-- Canada: double (nullable = true)\n |-- USA: double (nullable = true)\n |-- Norway: double (nullable = true)\n |-- Russia: double (nullable = true)\n |-- Netherlands: double (nullable = true)\n |-- Algeria: double (nullable = true)\n |-- Australia: double (nullable = true)\n |-- lng_year: integer (nullable = true)\n\n"}]},"apps":[],"jobName":"paragraph_1670772896969_-1770129663","id":"20221211-153456_1068187796","dateCreated":"2022-12-11T15:34:56+0000","dateStarted":"2022-12-11T17:55:12+0000","dateFinished":"2022-12-11T17:55:12+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9080"},{"text":"%pyspark\nimport os\nimport sys\n\nos.environ['PYSPARK_PYTHON'] = sys.executable\nos.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable","user":"anonymous","dateUpdated":"2022-12-11T17:55:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1670778331910_758192081","id":"20221211-170531_2036194388","dateCreated":"2022-12-11T17:05:31+0000","dateStarted":"2022-12-11T17:55:15+0000","dateFinished":"2022-12-11T17:55:15+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:9081"},{"text":"%pyspark\nmain.coalesce(1).write.option(\"header\",\"true\").format(\"csv\").save(\"hdfs:///user/maria_dev/pre/model\")","user":"anonymous","dateUpdated":"2022-12-11T17:55:37+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o1958.save.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:656)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:268)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:250)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:250)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:206)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:166)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:80)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.CoalesceExec.doExecute(basicPhysicalOperators.scala:583)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:180)\n\t... 30 more\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o1958.save.\\n', JavaObject id=o1959), <traceback object at 0x7f2791de6e18>)"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=201","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=202","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=203","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=204","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=205","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=206"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1670773807369_1042609570","id":"20221211-155007_584175831","dateCreated":"2022-12-11T15:50:07+0000","dateStarted":"2022-12-11T17:55:37+0000","dateFinished":"2022-12-11T18:00:44+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9082"},{"text":"%pyspark\nmain.show(2)","user":"anonymous","dateUpdated":"2022-12-11T17:19:28+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Py4JJavaError: An error occurred while calling o628.showString.\n: java.util.concurrent.TimeoutException: Futures timed out after [300 seconds]\n\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)\n\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)\n\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:201)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:136)\n\tat org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:367)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:144)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:140)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:135)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:268)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:250)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:250)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:102)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:65)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:206)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:181)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.consume(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.produceRows(ColumnarBatchScan.scala:166)\n\tat org.apache.spark.sql.execution.ColumnarBatchScan$class.doProduce(ColumnarBatchScan.scala:80)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doProduce(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.produce(DataSourceScanExec.scala:158)\n\tat org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:125)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:85)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:97)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:39)\n\tat org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:45)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:88)\n\tat org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:83)\n\tat org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:35)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:524)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:576)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:247)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:337)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)\n\tat org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3278)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3259)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3258)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2489)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2703)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:254)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n\n(<class 'py4j.protocol.Py4JJavaError'>, Py4JJavaError(u'An error occurred while calling o628.showString.\\n', JavaObject id=o1140), <traceback object at 0x7f2792b872d8>)"}]},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=99","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=100","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=101","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=102","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=103","http://sandbox-hdp.hortonworks.com:4040/jobs/job?id=104"],"interpreterSettingId":"spark2"}},"apps":[],"jobName":"paragraph_1670777291601_1327735349","id":"20221211-164811_770171418","dateCreated":"2022-12-11T16:48:11+0000","dateStarted":"2022-12-11T17:19:29+0000","dateFinished":"2022-12-11T17:24:32+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:9083"},{"text":"%pyspark\n","user":"anonymous","dateUpdated":"2022-12-11T17:00:38+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"python","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1670778038886_402408868","id":"20221211-170038_1602789407","dateCreated":"2022-12-11T17:00:38+0000","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:9084"}],"name":"merge_csv","id":"2HKG31GY4","noteParams":{},"noteForms":{},"angularObjects":{"spark2:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}